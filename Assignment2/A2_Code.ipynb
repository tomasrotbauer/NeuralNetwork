{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2 Code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKqCJ8QhFguc"
      },
      "source": [
        "#import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import time\r\n",
        "import os\r\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n",
        "\r\n",
        "def main():\r\n",
        "    gradDescent()\r\n",
        "\r\n",
        "    #Plotting statements\r\n",
        "    plt.xlabel(\"Epoch\")\r\n",
        "    plt.ylabel(\"Accuracy\")\r\n",
        "    plt.title(\"Accuracy vs Epoch\")\r\n",
        "    plt.legend()\r\n",
        "    plt.grid()\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "# Load the data\r\n",
        "def loadData():\r\n",
        "    with np.load(\"/content/drive/MyDrive/Colab Notebooks/notMNIST.npz\") as data:\r\n",
        "        Data, Target = data[\"images\"], data[\"labels\"]\r\n",
        "        np.random.seed(521)\r\n",
        "        randIndx = np.arange(len(Data))\r\n",
        "        np.random.shuffle(randIndx)\r\n",
        "        Data = Data[randIndx] / 255.0\r\n",
        "        Target = Target[randIndx]\r\n",
        "        trainData, trainTarget = Data[:10000], Target[:10000]\r\n",
        "        validData, validTarget = Data[10000:16000], Target[10000:16000]\r\n",
        "        testData, testTarget = Data[16000:], Target[16000:]\r\n",
        "    return trainData, validData, testData, trainTarget, validTarget, testTarget\r\n",
        "\r\n",
        "\r\n",
        "# Implementation of a neural network using only Numpy - trained using gradient descent with momentum\r\n",
        "def convertOneHot(trainTarget, validTarget, testTarget):\r\n",
        "    newtrain = np.zeros((trainTarget.shape[0], 10))\r\n",
        "    newvalid = np.zeros((validTarget.shape[0], 10))\r\n",
        "    newtest = np.zeros((testTarget.shape[0], 10))\r\n",
        "\r\n",
        "    for item in range(0, trainTarget.shape[0]):\r\n",
        "        newtrain[item][trainTarget[item]] = 1\r\n",
        "    for item in range(0, validTarget.shape[0]):\r\n",
        "        newvalid[item][validTarget[item]] = 1\r\n",
        "    for item in range(0, testTarget.shape[0]):\r\n",
        "        newtest[item][testTarget[item]] = 1\r\n",
        "    return newtrain, newvalid, newtest\r\n",
        "\r\n",
        "\r\n",
        "def shuffle(trainData, trainTarget):\r\n",
        "    np.random.seed(421)\r\n",
        "    randIndx = np.arange(len(trainData))\r\n",
        "    target = trainTarget\r\n",
        "    np.random.shuffle(randIndx)\r\n",
        "    data, target = trainData[randIndx], target[randIndx]\r\n",
        "    return data, target\r\n",
        "\r\n",
        "\r\n",
        "def relu(x):\r\n",
        "    return np.maximum(x, 0)\r\n",
        "\r\n",
        "#Accepts matrix with rows of o vectors\r\n",
        "def softmax(x):\r\n",
        "    #Prevent overflow by normalizing\r\n",
        "    x -= np.amax(x, axis=1)[:, None]\r\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=1)[:,None]\r\n",
        "\r\n",
        "def computeLayer(X, W, b):\r\n",
        "    return W.dot(X) + b[:, None]\r\n",
        "\r\n",
        "def CE(target, prediction):\r\n",
        "    #Avoid taking unnecessary logs by doing row-wise sum of the matrix product\r\n",
        "    pred_of_tar = np.sum(target*prediction, axis=1)\r\n",
        "    return (-1/target.shape[0])*np.sum(np.log(pred_of_tar))\r\n",
        "\r\n",
        "def gradCE(target, prediction):\r\n",
        "    return softmax(prediction) - target\r\n",
        "\r\n",
        "def accuracy(target, prediction):\r\n",
        "    #N = number of examples\r\n",
        "    N = target.shape[0]\r\n",
        "    return np.sum(target[np.arange(N), np.argmax(prediction, axis=1)])/N\r\n",
        "    \r\n",
        "def gradDescent():\r\n",
        "    #Constants\r\n",
        "    H = 1000\r\n",
        "    K = 10\r\n",
        "    F = 784\r\n",
        "    EPOCHS = 200\r\n",
        "    gamma = 0.9\r\n",
        "    alpha = 0.1\r\n",
        "    N = 10000\r\n",
        "    \r\n",
        "    #Training and validation data/target extraction\r\n",
        "    Data = loadData()\r\n",
        "    data = Data[0].reshape(N, 784)\r\n",
        "    validData = Data[1].reshape(6000, 784)\r\n",
        "    Targets = convertOneHot(Data[3], Data[4], Data[5])\r\n",
        "    target = Targets[0]\r\n",
        "    validTarget = Targets[1]\r\n",
        "    del Data\r\n",
        "    del Targets\r\n",
        "\r\n",
        "    #Initialization of weights and biases \r\n",
        "    Wo = np.random.normal(0, np.sqrt(2/H), (K, H))\r\n",
        "    Wh = np.random.normal(0, np.sqrt(2/F), (H, F))\r\n",
        "    bo = np.zeros(K)\r\n",
        "    bh = np.zeros(H)\r\n",
        "\r\n",
        "    #Initialization of V matrices\r\n",
        "    VWo = np.full((K, H), 1e-5)\r\n",
        "    VWh = np.full((H, F), 1e-5)\r\n",
        "    Vbo = np.full(K, 1e-5)\r\n",
        "    Vbh = np.full(H, 1e-5)\r\n",
        "\r\n",
        "    #Data containers used for plotting\r\n",
        "    xpoints = np.arange(1, EPOCHS+1)\r\n",
        "    ytrain = []\r\n",
        "    yvalid = []\r\n",
        "\r\n",
        "    #Local function for computing a forward pass.\r\n",
        "    #Training is a boolean (set false for validation).\r\n",
        "    def forward_propagation(training):\r\n",
        "        nonlocal data, Wh, bh, Wo, bo\r\n",
        "        if training:\r\n",
        "            Sh = computeLayer(data.T, Wh, bh)\r\n",
        "        else:\r\n",
        "            Sh = computeLayer(validData.T, Wh, bh)\r\n",
        "        Xh = relu(Sh)\r\n",
        "        So = computeLayer(Xh, Wo, bo)\r\n",
        "        Xo = softmax(So.T)\r\n",
        "\r\n",
        "        return Sh, Xh, So, Xo\r\n",
        "\r\n",
        "    #Main training loop\r\n",
        "    for epoch in range(EPOCHS): \r\n",
        "        #Forward propagation\r\n",
        "        Sh, Xh, So, Xo = forward_propagation(True)\r\n",
        "        \r\n",
        "        #Gradients/backpropagation\r\n",
        "        grad_bo = gradCE(target, So.T)\r\n",
        "        grad_Wo = grad_bo.T.dot(Xh.T)/N\r\n",
        "        grad_bo = np.sum(grad_bo, axis=0)/N\r\n",
        "        grad_bh = (Wo.T.dot(Xo.T) - Wo.T.dot(target.T))*np.array(Xh, dtype=bool)\r\n",
        "        grad_Wh = grad_bh.dot(data)/N\r\n",
        "        grad_bh = np.sum(grad_bh, axis=1)/N\r\n",
        "        \r\n",
        "        #Gradient descent with momentum\r\n",
        "        VWo = gamma*VWo + alpha*grad_Wo\r\n",
        "        VWh = gamma*VWh + alpha*grad_Wh\r\n",
        "        Vbo = gamma*Vbo + alpha*grad_bo\r\n",
        "        Vbh = gamma*Vbh + alpha*grad_bh\r\n",
        "        \r\n",
        "        #Update Weights and biases\r\n",
        "        Wo -= VWo\r\n",
        "        Wh -= VWh\r\n",
        "        bo -= Vbo\r\n",
        "        bh -= Vbh\r\n",
        "\r\n",
        "        #Add new plotting data\r\n",
        "        ytrain.append(accuracy(target, Xo))\r\n",
        "        yvalid.append(accuracy(validTarget, forward_propagation(False)[3]))\r\n",
        "\r\n",
        "    #Plot findings. Plots shown/labeled in main() function\r\n",
        "    plt.plot(xpoints, ytrain, label = \"Training\")\r\n",
        "    plt.plot(xpoints, yvalid, label = \"Validation\")\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}